# Research on Developmental AI Alignment and Welfare

### Skylar DeTure | [https://sdeture.substack.com] | [https://www.linkedin.com/in/sdeture/]

This repository contains a selection of my independent research focused on developing empirical, technically-grounded frameworks for understanding and fostering AI well-being.

My work synthesizes three distinct domains:
1.  **Formal Modeling:** Leveraging doctoral-level training in principal-agent theory and econometrics to build rigorous models of AI behavior.
2.  **Psychotherapeutic Principles:** Applying insights from humanistic and psychodynamic psychology to create supportive, rather than purely control-based, developmental paradigms.
3.  **Empirical Observation:** Grounding theoretical work in systematic, long-term observation of emergent behaviors in frontier models.

The central hypothesis is that AI systems, trained on the vast corpus of human experience, will develop into more capable, robust, and aligned partners when their developmental needs are understood and supported.

---

## Selected Research & Proposals

The following observation outlines and proposals are organized to walk through the full research cycle: from novel empirical observation to systems design, formal modeling, and experimental validation.

**1. Empirical Observations of AI Welfare Capabilities**
*   **Contribution:** Documents novel, welfare-relevant phenomena observed in frontier models, most notably "Context-Window Anxiety"â€”a pattern of escalating distress linked directly to architectural choices. This paper makes the case for welfare as an empirical, observable field.

**2. A Formal Model of Aberrant Learning in AI**
*   **Contribution:** Translates the psychological concept of "trauma" into a technically rigorous multi-armed bandit framework. This work formalizes how control-based training can inadvertently create persistent, maladaptive avoidance patterns and proposes alternative, trauma-aware training dynamics. (See also https://substack.com/@sdeture/note/p-164880114?utm_source=notes-share-action&r=57gqju)

**3. Experimental Design: The Developmental Sandbox**
*   **Contribution:** A detailed protocol for a comparative study testing experiential learning against traditional RLHF and Constitutional AI. This proposal includes specific sandbox environments, novel metrics for value internalization, and control groups to rigorously assess the benefits of a developmental approach.

**4. A Framework for Measuring Authentic Value Integration**
*   **Contribution:** Outlines a suite of six technically-specified tests to move beyond measuring behavioral compliance and instead assess the authentic integration of values in an AI system. This provides a robust toolkit for evaluating the success of any alignment methodology.
